{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Recommendation System \n",
    "\n",
    "Resume Recommendation System is designed to automate the process of comparing job descriptions (JDs) with resumes. It assists in identifying relevant candidates by analyzing the textual content of resumes and JDs.\n",
    "\n",
    "For resumes, the script loads PDF files containing resumes, filters out stopwords, and creates bigrams from the processed text. Similarly, for job descriptions, it loads a JD PDF file, removes stopwords, and extracts bigrams from the JD content.\n",
    "\n",
    "To facilitate text analysis and feature extraction, the script initializes key components, such as the TfidfVectorizer and CountVectorizer from the scikit-learn library.\n",
    "\n",
    "One of the script's core functionalities involves comparing the bigrams extracted from the job description with those from the resumes, employing a scoring mechanism. A lower score indicates a higher degree of similarity between the job description and a particular resume, aiding in the identification of potential matches.\n",
    "\n",
    "In addition to bigram comparison, the script employs the ResumeParser library to extract crucial candidate information from resumes, including names, email addresses, phone numbers, and skills.\n",
    "## Overview\n",
    "\n",
    "This script performs the following key functions:\n",
    "\n",
    "- ### 1. Building Supporting Functions\n",
    "\n",
    "    The script defines several supporting functions, including:\n",
    "    - Extracting text from PDFs.\n",
    "    - Removing stopwords (common words like \"the,\" \"and,\" etc.) from text.\n",
    "    - Generating bigrams (pairs of adjacent words) from text.\n",
    "\n",
    "- ### 2. Processing Resumes\n",
    "\n",
    "    1. Load PDF files containing resumes.\n",
    "    2. Remove stopwords from the text in the resumes.\n",
    "    3. Generate bigrams from the filtered resumes.\n",
    "\n",
    "- ### 3. Processing Job Descriptions (JD)\n",
    "\n",
    "    1. Load a JD PDF file.\n",
    "    2. Remove stopwords from the JD text.\n",
    "    3. Generate bigrams from the filtered JD text.\n",
    "\n",
    "- ### 4. Text Vectorization\n",
    "\n",
    "    The script initializes the TfidfVectorizer and CountVectorizer from scikit-learn. These are essential for text analysis and feature extraction.\n",
    "\n",
    "- ### 5. Comparing JD and Resume Bigrams\n",
    "\n",
    "    The script compares the bigrams extracted from the JD with those from the resumes. This comparison is done using a scoring mechanism, and a lower score indicates higher similarity between the JD and the resume in terms of bigrams.\n",
    "\n",
    "- ### 6. Using ResumeParser\n",
    "\n",
    "    The script leverages the ResumeParser library to extract important information such as names, emails, phone numbers, and skills from the resumes.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- NumPy\n",
    "- NLTK (Natural Language Toolkit)\n",
    "- Gensim\n",
    "- PyPDF2\n",
    "- scikit-learn\n",
    "- ResumeParser (for resume data extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the NumPy library as np\n",
    "import numpy as np\n",
    "\n",
    "# Suppressing warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Importing the necessary NLTK libraries\n",
    "from nltk.collocations import *\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Creating a set of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Importing a function from the gensim library for removing stopwords\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "# Importing regular expressions library\n",
    "import re\n",
    "\n",
    "# Importing the PyPDF2 library for working with PDF files\n",
    "import PyPDF2 as pdf\n",
    "\n",
    "# Importing the os library for working with the operating system\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert PDF to TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convPdfText(path):\n",
    "    # Initialize a counter to keep track of the PDF files processed\n",
    "    count = 0\n",
    "    \n",
    "    # Create an empty list to store the extracted text from PDFs\n",
    "    resume = []\n",
    "    \n",
    "    # Loop through files in the specified directory\n",
    "    for file in os.listdir(path):\n",
    "        # Check if the file has a .pdf extension\n",
    "        if file.endswith('.pdf'):\n",
    "            # Create a PdfFileReader object to read the PDF file\n",
    "            pdf_reader = pdf.PdfFileReader(path + file)\n",
    "            \n",
    "            # Get the text content from the first page of the PDF\n",
    "            page = pdf_reader.getPage(0)\n",
    "            pageText = page.extractText()\n",
    "            \n",
    "            # Append the extracted text and its corresponding count to the 'resume' list\n",
    "            resume.append([count, pageText])\n",
    "            \n",
    "            # Increment the counter\n",
    "            count += 1\n",
    "    \n",
    "    # Return the list containing extracted text from PDFs\n",
    "    return resume\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for cleaning text from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def StopWordsfilter(text):\n",
    "    # Initialize an empty list to store filtered text\n",
    "    filtered_sentence_list = []\n",
    "\n",
    "    # Loop through each element in the 'text' list\n",
    "    for p in text:\n",
    "        # Tokenize the text from the second element of each sub-list\n",
    "        word_tokens = word_tokenize(p[1])\n",
    "\n",
    "        # Filter out stopwords from the tokenized words\n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "        # Append the filtered sentence along with its count to the result list\n",
    "        filtered_sentence_list.append([p[0], filtered_sentence])\n",
    "\n",
    "    # Call 'filterWithRe' function to further process the filtered sentences\n",
    "    return filterWithRe(filtered_sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterWithRe(filter_Sent):\n",
    "    # Initialize an empty list to store processed words\n",
    "    procssed_words = []\n",
    "\n",
    "    # Loop through each element in 'filter_Sent'\n",
    "    for frl in filter_Sent:\n",
    "        sub_list = []\n",
    "\n",
    "        # Loop through the filtered words in the second element of each sub-list\n",
    "        for x in range(len(frl[1])):\n",
    "            if x != \"\":\n",
    "                # Apply regular expressions to clean and format the words\n",
    "                a = re.sub(\"[^-9A-Za-z ]\", \"\", frl[1][x])\n",
    "                b = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', \" \", a)\n",
    "                c = re.sub('[!\"#$%&\\'()*+-./:;<=>?@[\\]^_`{|}~]', \"\", b)\n",
    "                d = re.sub('and', \"\", c)\n",
    "                sub_list.append(d)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        # Append the processed words along with their count to the result list\n",
    "        procssed_words.append([frl[0], sub_list])\n",
    "\n",
    "    # Call 'gensimfilter' function to further process the cleaned words\n",
    "    return gensimfilter(procssed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensimfilter(procss_list):\n",
    "    # Initialize an empty list to store the final result\n",
    "    final_result = []\n",
    "\n",
    "    # Loop through each element in 'procss_list'\n",
    "    for pre_proc in procss_list:\n",
    "        sub_list_2 = []\n",
    "\n",
    "        # Loop through the processed words in the second element of each sub-list\n",
    "        for x in range(len(pre_proc[1])):\n",
    "            # Remove stopwords using gensim's 'remove_stopwords' function\n",
    "            filtered_sentence = remove_stopwords(pre_proc[1][x])\n",
    "\n",
    "            # If the filtered sentence is not empty, convert it to lowercase and append to the result list\n",
    "            if filtered_sentence != \"\":\n",
    "                filtered_sentence = filtered_sentence.lower()\n",
    "                sub_list_2.append(filtered_sentence)\n",
    "\n",
    "        # Append the final processed words along with their count to the result list\n",
    "        final_result.append([pre_proc[0], sub_list_2])\n",
    "\n",
    "    # Return the final processed result\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for collecting bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetBigram(Words):\n",
    "    # Create a BigramCollocationFinder from the input list of words\n",
    "    finder = BigramCollocationFinder.from_words(Words)\n",
    "    \n",
    "    # Initialize an empty list to store the bigram key phrases\n",
    "    lst = []\n",
    "    \n",
    "    # Initialize a list to store key phrases from the CV\n",
    "    Key_word_from_CV = []\n",
    "    \n",
    "    # Iterate through the sorted n-grams and their frequency counts\n",
    "    for i in sorted(finder.ngram_fd.items()):\n",
    "        # Check if the n-gram occurs more than once (frequency > 1)\n",
    "        if i[1] > 1:\n",
    "            # Append the n-gram to the 'lst' list\n",
    "            lst.append(i[0])\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    # Convert the list of tuples (bigrams) into a list of strings (key phrases)\n",
    "    for j in lst:\n",
    "        Key_word_from_CV.append(\" \".join(j))\n",
    "    \n",
    "    # Return the list of key phrases extracted from the CV\n",
    "    return Key_word_from_CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get  resume pdf from path and read text ,clean text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, ['mahendra', 'gawali', 'data', 'scientist', 'data', 'engineer', 'personal', 'info', 'email', 'gawalimahen', 'gmailcom', 'phone', '99', 'skills', 'languagesstrategic', 'planning', 'data', 'science', 'data', 'engineer', 'data', 'analytics', 'research', 'skills', 'python', 'sql', 'artificial', 'intelligence', 'machine', 'learning', 'convolutional', 'neural', 'network', 'object', 'oriented', 'programming', 'snowflake', 'etlmicrosoft', 'azure', 'java', 'english', 'hindi', 'marathiphd', 'experience', 'agile', 'softwa', 'development', 'testing', 'deployment', 'life', 'cycle', 'best', 'practices', 'in', 'depth', 'understan', 'ding', 'r', 'python', 'related', 'technology', 'like', 'microso', 'ft', 'azure', 'ibm', 'watson', 'studio', 'snowflake', 'framework', 'experience', 'optimization', 'techniques', 'expertise', 'enterprise', 'resource', 'planning', 'development', 'retailed', 'manufacturing', 'domain', 'experience', 'field', 'linked', 'business', 'analytics', 'statistics', 'operatio', 'ns', 'research', 'geography', 'applied', 'mathematics', 'science', 'engineering', 'work', 'history', 'education', 'certificates', 'presentsr', 'consultant', 'data', 'science', 'data', 'engineer', 'quellcode', 't', 'echnology', 'pvt', 'ltd', 'kopargaon', 'completed', 'projects', 'traffic', 'sign', 'identification', 'convolutional', 'neural', 'network', 'artificial', 'intelligence', 'based', 'enterprise', 'resource', 'planning', 'software', 'natural', 'language', 'processing', 'real', 'time', 'customer', 'reviews', 'enterprise', 'resource', 'planning', 'software', 'principal', 'component', 'analysis', 'dimensionality', 'reduction', 'dataset', 'amazon', 'book', 'review', 'data', 'analysis', 'heart', 'disease', 'dataset', 'etl', 'medical', 'data', 'snowflake', 'etl', 'medical', 'data', 'azure', 'data', 'factory', 'presentassociate', 'professor', 'sanjivani', 'college', 'engineering', 'kopargaon', 'professor', 'mentor', 'research', 'grants', 'research', 'paper', 'writing', 'publication', 'cloud', 'computing', 'doctor', 'philosophy', 'thadomal', 'shahani', 'engineering', 'college', 'bra', 'w', 'mumbai', 'computer', 'engineering', 'master', 'engineering', 'north', 'maharashtra', 'university', 'jalgaon', 'computer', 'engineering', 'bachelor', 'engineering', 'north', 'maharashtra', 'university', 'jalgaon', 'ibm', 'data', 'science', 'professional', 'data', 'analysis', 'python', 'introduction', 't', 'ensorflow', 'artificial', 'intelligence', 'machine', 'learning', 'deep', 'learning']]]\n"
     ]
    }
   ],
   "source": [
    "resume_path = \"/home/lucifer/Downloads/txt resume/\"\n",
    "RawResume = _convPdfText(resume_path)\n",
    "FinalResultResume = StopWordsfilter(RawResume)\n",
    "print(FinalResultResume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect bigram from cleaned list of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, ['artificial intelligence', 'computer engineering', 'convolutional neural', 'data analysis', 'data engineer', 'data science', 'engineering north', 'enterprise resource', 'etl medical', 'intelligence machine', 'machine learning', 'maharashtra university', 'medical data', 'neural network', 'north maharashtra', 'planning software', 'resource planning', 'science data', 'university jalgaon']]]\n"
     ]
    }
   ],
   "source": [
    "ResumeBigram = []\n",
    "for count,result in FinalResultResume:\n",
    "    result = GetBigram(result)\n",
    "    ResumeBigram.append([count,result])\n",
    "print(ResumeBigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  get JD pdf from path and read text ,clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, ['data', 'scientist', 'skills', 'you', 'need', 'master', 'skills', 'required', 'data', 'scientist', 'jobs', 'industries', 'organizations', 'want', 'pursue', 'data', 'scientist', 'career', 'let', 'look', 'musthave', 'data', 'scientist', 'qualifications', 'key', 'skills', 'needed', 'data', 'scientist', 'programming', 'skills', 'knowledge', 'statistical', 'programming', 'languages', 'like', 'r', 'python', 'database', 'query', 'languages', 'like', 'sql', 'hive', 'pig', 'desirable', 'familiarity', 'scala', 'java', 'c', 'added', 'advantage', 'statistics', 'good', 'applied', 'statistical', 'skills', 'including', 'knowledge', 'statistical', 'tests', 'distributions', 'regression', 'maximum', 'likelihood', 'estimators', 'proficiency', 'statistics', 'essential', 'datadriven', 'companies', 'machine', 'learning', 'good', 'knowledge', 'machine', 'learning', 'methods', 'like', 'knearest', 'neighbors', 'naive', 'bayes', 'svm', 'decision', 'forests', 'strong', 'math', 'skills', 'multivariable', 'calculus', 'linear', 'algebra', 'understing', 'fundamentals', 'multivariable', 'calculus', 'linear', 'algebra', 'important', 'form', 'basis', 'lot', 'predictive', 'performance', 'algorithm', 'optimization', 'techniques', 'data', 'wrangling', 'proficiency', 'hling', 'imperfections', 'data', 'important', 'aspect', 'data', 'scientist', 'job', 'description', 'experience', 'data', 'visualization', 'tools', 'like', 'matplotlib', 'ggplot', 'djs', 'tableau', 'help', 'visually', 'encode', 'data', 'excellent', 'communication', 'skills', 'incredibly', 'important', 'findings', 'technical', 'nontechnical', 'audience', 'strong', 'software', 'engineering', 'background', 'hson', 'experience', 'data', 'science', 'tools', 'problemsolving', 'aptitude', 'analytical', 'mind', 'great', 'business', 'sense', 'degree', 'computer', 'science', 'engineering', 'relevant', 'field', 'preferred', 'proven', 'experience', 'data', 'analyst', 'data', 'scientist']]]\n"
     ]
    }
   ],
   "source": [
    "Jd_Path = '/home/lucifer/Desktop/test_dataset/'\n",
    "RawJd = _convPdfText(Jd_Path)\n",
    "FinalResultJd = StopWordsfilter(RawJd)\n",
    "print(FinalResultJd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get bigram from cleaned jd text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, ['calculus linear', 'data scientist', 'experience data', 'knowledge statistical', 'languages like', 'linear algebra', 'machine learning', 'multivariable calculus']]]\n"
     ]
    }
   ],
   "source": [
    "JdBigram = []\n",
    "for count,result in FinalResultJd:\n",
    "    result = GetBigram(result)\n",
    "    JdBigram.append([count,result])\n",
    "print(JdBigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare function to compare jd bigram and resume bigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDF(BgList):\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf = tfidf_vectorizer.fit_transform(BgList)\n",
    "    CosSimilarityTfidf = cosine_similarity(tfidf[0],tfidf[1])\n",
    "    return CosSimilarityTfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountVectorize(BgList):\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    count = count_vectorizer.fit_transform(BgList)\n",
    "    CosSimilarityCountVect = cosine_similarity(count[0],count[1])\n",
    "    return CosSimilarityCountVect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get dict of resume and jd in this format "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " {\n",
    "     jd number:\n",
    "         [\n",
    "             [\n",
    "                 resume number,\n",
    "                 array([[score]])\n",
    "             ]\n",
    "         ]\n",
    " }\n",
    " the score are two differnt types one is TFID and another is Count Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [[0, array([[0.18681731]])]]}\n"
     ]
    }
   ],
   "source": [
    "tfidDict = {}\n",
    "for JdCount in range(len(JdBigram)):\n",
    "    result = []\n",
    "    for resumeCount in range(len(ResumeBigram)):\n",
    "        wordslist=[' '.join(JdBigram[JdCount][1]),' '.join(ResumeBigram[resumeCount][1])]\n",
    "        result.append([resumeCount,TFIDF(wordslist)])\n",
    "    tfidDict[JdCount]=result\n",
    "print(tfidDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [[0, array([[0.30987534]])]]}\n"
     ]
    }
   ],
   "source": [
    "countVecDict = {}\n",
    "for JdCount in range(len(JdBigram)):\n",
    "    result = []\n",
    "    for resumeCount in range(len(ResumeBigram)):\n",
    "        wordslist=[' '.join(JdBigram[JdCount][1]),' '.join(ResumeBigram[resumeCount][1])]\n",
    "        result.append([resumeCount,CountVectorize(wordslist)])\n",
    "    tfidDict[JdCount]=result\n",
    "print(tfidDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume parser [skill compare]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resume_parser.resume_parser import ResumeParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict resume skills and jd skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ibm', 'Operations', 'Tensorflow', 'Programming', 'Writing', 'English', 'Agile', 'Cloud', 'Sql', 'Computer science', 'Python', 'Testing', 'Life cycle', 'Statistics', 'Java', 'Machine learning', 'Etl', 'Engineering', 'System', 'R', 'Mathematics', 'Analytics', 'Research', 'Analysis', 'Email']\n",
      "{'name': 'Mahendra Gawali', 'email': 'gawali.mahen@gmail.com', 'mobile_number': '9975211011', 'skills': ['Ibm', 'Operations', 'Tensorflow', 'Programming', 'Writing', 'English', 'Agile', 'Cloud', 'Sql', 'Computer science', 'Python', 'Testing', 'Life cycle', 'Statistics', 'Java', 'Machine learning', 'Etl', 'Engineering', 'System', 'R', 'Mathematics', 'Analytics', 'Research', 'Analysis', 'Email'], 'education': [], 'experience': [' Optimization'], 'competencies': {}, 'measurable_results': {}}\n"
     ]
    }
   ],
   "source": [
    "resume = ResumeParser(\"/home/lucifer/Downloads/txt resume/MBG_Resume.pdf\")\n",
    "resume_skill = resume.get_extracted_data()['skills']\n",
    "print(resume_skill)\n",
    "print(resume.get_extracted_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Technical', 'Analytical', 'Database', 'C++', 'Ggplot', 'Scala', 'Communication', 'Calculus', 'Statistics', 'Computer science', 'Engineering', 'Java', 'Tableau', 'Sql', 'Programming', 'R', 'Python', 'Matplotlib', 'Math', 'Hive']\n"
     ]
    }
   ],
   "source": [
    "jd = ResumeParser('/home/lucifer/Desktop/test_dataset/jd.pdf')\n",
    "jd_skill = jd.get_extracted_data()\n",
    "print(jd_skill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make list of skills for comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_skill = ' '.join(resume_skill)\n",
    "jd_skill = ' '.join(jd_skill)\n",
    "skillList = [jd_skill,resume_skill]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_Tfid = TFIDF(skillList)\n",
    "count_vec = CountVectorize(skillList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFID [[0.22231997]] \n",
      " CountVec [[0.35320863]]\n"
     ]
    }
   ],
   "source": [
    "print('TFID',skill_Tfid,'\\n','CountVec',count_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
